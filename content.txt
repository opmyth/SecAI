
==================================================
File: main.py
==================================================

import gradio as gr
from utils.ui import *
from utils.utils import process_input


def create_interface():
    with gr.Blocks(theme=gr.themes.Soft()) as demo:
        header_components = create_header("utils/sdaiaAcademyLogo.png")

        processing = gr.State(True)
        current_meetings = gr.State([])
        email_summary = gr.State("")
        content_summary = gr.State("")

        with gr.Row():
            with gr.Column():
                audio_input, audio_output, camera_input, reset_btn = create_input_column()

            with gr.Column():
                calendar_table, email_output, content_output = create_output_column()

        create_footer_instructions()

        def reset():
            return None, "", "", True, [], "", ""

        audio_input.change(
            fn=lambda *args: process_input(args[0], "audio", *args[1:]),
            inputs=[
                audio_input,
                processing,
                current_meetings,
                email_summary,
                content_summary,
            ],
            outputs=[
                audio_output,
                calendar_table,
                email_output,
                content_output,
                processing,
                current_meetings,
                email_summary,
                content_summary,
            ],
        )

        camera_input.stream(
            fn=lambda *args: process_input(args[0], "gesture", *args[1:]),
            inputs=[
                camera_input,
                processing,
                current_meetings,
                email_summary,
                content_summary,
            ],
            outputs=[
                audio_output,
                calendar_table,
                email_output,
                content_output,
                processing,
                current_meetings,
                email_summary,
                content_summary,
            ],
        )

        reset_btn.click(
            fn=reset,
            outputs=[
                calendar_table,
                email_output,
                content_output,
                processing,
                current_meetings,
                email_summary,
                content_summary,
            ],
        )

    return demo


if __name__ == "__main__":
    demo = create_interface()
    demo.launch()


==================================================
File: utils/ui.py
==================================================

import gradio as gr
import base64

def get_base64_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def create_header(logo_path):
    logo = get_base64_image(logo_path)
    instructions_header = """
    ### Welcome to secAI: Your AI Secretary
    
    secAI helps you manage your tasks using voice commands or hand gestures.
    """
    
    return [
        gr.HTML(f"""
            <style>
                .header {{
                    display: flex;
                    justify-content: space-between;
                    align-items: center;
                    padding: 20px;
                    background: white;
                }}
                .content {{ padding: 20px; }}
            </style>
            <div class="header">
                <h1>secAI: Your AI Secretary</h1>
                <img src="data:image/png;base64,{logo}" alt="Logo" width="350">
            </div>
        """),
        gr.Markdown(instructions_header)
    ]

def create_footer_instructions():
    instructions = """
    ### How to Use secAI

    #### Voice Commands
    1. Click the microphone icon to start recording
    2. Speak your command clearly:
       - Say "Show my emails" or "Check my inbox" for email summary
       - Say "Show my calendar" or "What meetings do I have today" for calendar
       - Say "Summarize content" or "Get latest papers" for content summary
    3. Wait for the system to process your request

    #### Hand Gestures
    Use these gestures in front of your camera:
    - ‚òùÔ∏è One finger - Show a summary of your emails
    - ‚úåÔ∏è Two fingers - Show the meetings in your calendar
    - üëå Three fingers - Summarize the latest papers in computer vision

    #### Additional Notes
    - Click "Start New Task" to reset and start a new request
    - The transcribed text will show what the system heard from your voice command
    - Results will appear in the corresponding tab on the right
    """
    return gr.Markdown(instructions)

def create_input_column():
    audio_input = gr.Audio(sources="microphone", type="filepath", label="Voice Input")
    audio_output = gr.Text(label="Transcribed Text")
    camera_input = gr.Image(sources="webcam", streaming=True, label="Gesture Input", interactive=True)
    reset_btn = gr.Button("Start New Task")
    return audio_input, audio_output, camera_input, reset_btn  

def create_output_column():
    with gr.Tabs():
        with gr.TabItem("Email Summary"):
            email_output = gr.Markdown(label="Email Summary", value="")
            
        with gr.TabItem("Calendar"):
            calendar_table = gr.Dataframe(
                headers=["Time", "Meeting Title"],
                label="Today's Meetings",
                value=[],
                type="array"
            )
            
        with gr.TabItem("Content Summarizer"):
            content_output = gr.Markdown(label="Content Summary", value="")
            
    return calendar_table,email_output, content_output


==================================================
File: utils/utils.py
==================================================

import subprocess
from datetime import datetime
from modules.llm_processor import get_llm_output

def get_calendar_meetings():
    script = '''
    tell application "Calendar"
        set todayDate to current date
        -- Set start date to beginning of today
        set startDate to todayDate - (time of todayDate)
        -- Set end date to end of today
        set endDate to startDate + (24 * hours)
        
        set eventList to {}
        
        repeat with calendarAccount in calendars
            try
                set theEvents to (every event of calendarAccount whose start date ‚â• startDate and start date < endDate)
                
                repeat with anEvent in theEvents
                    set eventStart to start date of anEvent
                    set eventHour to text -2 thru -1 of ("0" & (hours of eventStart as integer))
                    set eventMinute to text -2 thru -1 of ("0" & (minutes of eventStart as integer))
                    set eventTime to eventHour & ":" & eventMinute
                    set eventTitle to summary of anEvent
                    
                    copy {eventTime, eventTitle} to end of eventList
                end repeat
            end try
        end repeat
        
        return eventList
    end tell
    '''
    
    try:
        process = subprocess.Popen(['osascript', '-e', script],
                                 stdout=subprocess.PIPE,
                                 stderr=subprocess.PIPE)
        
        output, error = process.communicate()
        
        meetings = []
        if output:
            output_str = output.decode('utf-8').strip()
            parts = output_str.split(', ')
            
            for i in range(0, len(parts), 2):
                if i + 1 < len(parts):
                    time = parts[i]
                    title = parts[i + 1]
                    meetings.append([time, title])
        
        meetings.sort(key=lambda x: x[0])
        print(f"Generated meetings: {meetings}")
        return meetings
        
    except Exception as e:
        print(f"Error accessing Calendar: {e}")
        return []

def get_last_10_emails():
    import os.path
    import base64
    import re
    from email.mime.text import MIMEText
    from google.auth.transport.requests import Request
    from google.oauth2.credentials import Credentials
    from google_auth_oauthlib.flow import InstalledAppFlow
    from googleapiclient.discovery import build
    from googleapiclient.errors import HttpError

    SCOPES = ["https://www.googleapis.com/auth/gmail.readonly"]
    output = []

    try:
        creds = None
        if os.path.exists("credentials/token.json"):
            creds = Credentials.from_authorized_user_file("credentials/token.json", SCOPES)
        
        if not creds or not creds.valid:
            if creds and creds.expired and creds.refresh_token:
                try:
                    creds.refresh(Request())
                except:
                    os.remove("credentials/token.json")
                    flow = InstalledAppFlow.from_client_secrets_file("credentials/credentials.json", SCOPES)
                    creds = flow.run_local_server(port=0)
            else:
                flow = InstalledAppFlow.from_client_secrets_file("credentials/credentials.json", SCOPES)
                creds = flow.run_local_server(port=0)
            
            with open("credentials/token.json", "w") as token:
                token.write(creds.to_json())

        service = build("gmail", "v1", credentials=creds)
        
        messages = service.users().messages().list(
            userId='me',
            maxResults=10,
            q="category:primary is:unread"
        ).execute()

        if 'messages' not in messages:
            return "No unread messages in Primary category."

        output.append("Latest 10 unread emails from Primary:")
        output.append("-" * 70)

        for i, message in enumerate(messages['messages'], 1):
            msg = service.users().messages().get(
                userId='me',
                id=message['id']
            ).execute()

            payload = msg['payload']
            headers = payload['headers']

            subject = ''
            sender = ''
            date = ''
            sender_email = ''
            for header in headers:
                if header['name'].lower() == 'subject':
                    subject = header['value']
                if header['name'].lower() == 'from':
                    sender = header['value']
                    email_match = re.search(r'<([^>]+)>', sender)
                    if email_match:
                        sender_email = email_match.group(1)
                    else:
                        sender_email = re.search(r'[\w\.-]+@[\w\.-]+\.\w+', sender)
                        sender_email = sender_email.group(0) if sender_email else sender
                if header['name'].lower() == 'date':
                    date = header['value']

            body = ""
            if 'parts' in payload:
                for part in payload['parts']:
                    if part['mimeType'] == 'text/plain':
                        if 'data' in part['body']:
                            body = base64.urlsafe_b64decode(part['body']['data']).decode('utf-8')
                            break
            else:
                if 'body' in payload and 'data' in payload['body']:
                    body = base64.urlsafe_b64decode(payload['body']['data']).decode('utf-8')

            if not body:
                body = "No plain text content available"

            output.append(f"\nEmail {i}:")
            output.append(f"From: {sender}")
            output.append(f"Email: {sender_email}")
            output.append(f"Date: {date}")
            output.append(f"Subject: {subject}")
            output.append("\nContent:")
            content_preview = body[:200]
            if len(body) > 200:
                content_preview += "..."
            output.append(content_preview)
            output.append("-" * 70)

        return "\n".join(output)

    except HttpError as error:
        return f"An error occurred: {error}"
    except Exception as e:
        return f"An unexpected error occurred: {e}"

def get_content(query="computer vision", max_papers=5):
    import arxiv
    import os
    import PyPDF2
    import re
    from datetime import datetime
    from urllib.request import urlretrieve
    
    def extract_introduction(pdf_path):
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                text = ""
                for i in range(min(3, len(reader.pages))):
                    text += reader.pages[i].extract_text()
                
                intro_match = re.search(r'(?i)(introduction|1\.?\s+introduction).*?(?=[2-9]\.|\n\s*[2-9]\.)', text, re.DOTALL)
                if intro_match:
                    return intro_match.group(0)
                return text[:1500]
        except Exception as e:
            return f"Error extracting text: {str(e)}"
    
    def clean_text(text):
        return ' '.join(text.split())
    
    client = arxiv.Client()
    search = arxiv.Search(
        query=f'cat:cs.CV AND "{query}"',
        max_results=max_papers * 2,
        sort_by=arxiv.SortCriterion.SubmittedDate
    )
    
    temp_dir = "temp_arxiv_papers"
    os.makedirs(temp_dir, exist_ok=True)
    
    paper_summaries = []
    processed_count = 0
    
    try:
        for paper in client.results(search):
            if processed_count >= max_papers:
                break
                
            if 'cs.CV' in paper.primary_category:
                pdf_path = os.path.join(temp_dir, f"{paper.get_short_id()}.pdf")
                
                try:
                    urlretrieve(paper.pdf_url, pdf_path)
                    
                    paper_info = (
                        f"Paper title: {paper.title}\n"
                        f"Authors: {', '.join(author.name for author in paper.authors)}\n"
                        f"Publication date: {paper.published.strftime('%Y-%m-%d')}\n"
                        f"Category: {paper.primary_category}\n"
                        f"Abstract: {clean_text(paper.summary)}\n"
                        f"Introduction: {clean_text(extract_introduction(pdf_path))}\n"
                        f"{'='*80}\n\n"
                    )
                    paper_summaries.append(paper_info)
                    processed_count += 1
                    
                except Exception as e:
                    print(f"Error processing {paper.title}: {str(e)}")
                finally:
                    if os.path.exists(pdf_path):
                        os.remove(pdf_path)
    
    finally:
        if os.path.exists(temp_dir):
            try:
                os.rmdir(temp_dir)
            except:
                pass
    
    final_output = (
        f"ArXiv Paper Summaries - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        f"Query: {query} (Computer Vision papers only)\n"
        f"{'='*80}\n\n"
    )
    final_output += ''.join(paper_summaries)
    
    return final_output

def process_input(input_data, input_type, is_processing, meetings, prev_email_summary, prev_content_summary):
    from modules.input_handlers import handle_gesture, handle_audio
    
    if not is_processing:
        return None, meetings if meetings else None, prev_email_summary, prev_content_summary, False, meetings, prev_email_summary, prev_content_summary

    if input_type == "gesture":
        task_num = handle_gesture(input_data, is_processing)
        transcribed_text = None
    else:
        task_num, transcribed_text = handle_audio(input_data, is_processing)

    _, calendar_data, email_output, content_summary, new_is_processing, new_meetings, new_email_summary, new_content_summary = execute_task(
        task_num, transcribed_text, prev_email_summary, meetings, prev_content_summary
    )
    
    return (transcribed_text, calendar_data, email_output, content_summary, 
            new_is_processing, new_meetings, new_email_summary, new_content_summary)

def execute_task(task_num, transcribed_text, prev_email_summary, meetings, prev_content_summary):
    if task_num == -1:
        return None, None, prev_email_summary, prev_content_summary, True, meetings, prev_email_summary, prev_content_summary
        
    if task_num == 1:
        new_emails = get_last_10_emails()
        llm_output = get_llm_output(new_emails, "email_summary")
        return None, None, llm_output, prev_content_summary, False, meetings, llm_output, prev_content_summary

    elif task_num == 2:
        new_meetings = get_calendar_meetings()
        meetings_data = [[str(meeting[0]), str(meeting[1])] for meeting in new_meetings]
        return None, meetings_data, prev_email_summary, prev_content_summary, False, meetings_data, prev_email_summary, prev_content_summary
    
    elif task_num == 3:
        content = get_content()
        summary = get_llm_output(content, "content_summarizer")
        return None, None, prev_email_summary, summary, False, meetings, prev_email_summary, summary
            
    return None, None, prev_email_summary, prev_content_summary, True, meetings, prev_email_summary, prev_content_summary


==================================================
File: modules/speech_to_text.py
==================================================

from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration
import numpy as np

def speech_to_text(audio_path):
    try:
        from openai import OpenAI
        client = OpenAI()
        
        with open(audio_path, "rb") as audio_file:
            transcription = client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file
            )
            
        return transcription.text
    
    except Exception as e:
        raise Exception(f"Error in speech to text conversion: {str(e)}")


==================================================
File: modules/hand_gesture.py
==================================================

import base64
from openai import OpenAI
from PIL import Image
import numpy as np
import io
import os
import json

def process_gesture(image):
    if isinstance(image, np.ndarray):
        image = Image.fromarray(image)
    
    buffered = io.BytesIO()
    image.save(buffered, format="JPEG")
    image_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
    
    client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))
    
    with open('/Users/hasan/Desktop/sdaia_bootcamp/secAI/utils/system_prompts.json', 'r') as f:
        SYSTEM_PROMPTS = json.load(f)
    system_prompt = SYSTEM_PROMPTS['hand_gesture']
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": system_prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{image_base64}"
                            }
                        }
                    ]
                }
            ]
        )
        
        gesture = response.choices[0].message.content.strip().lower()
        
        gesture_map = {
            'one': 1,
            'two': 2,
            'three': 3
        }
        
        return gesture_map.get(gesture, 0)
        
    except Exception as e:
        print(f"Error processing image: {str(e)}")
        return 0
    
# print(process_gesture(Image.open('/Users/hasan/Desktop/sdaia_bootcamp/secAI/image.jpg')))


==================================================
File: modules/llm_processor.py
==================================================

import os
import json


def get_llm_output(prompt: str, task_type: str) -> str:
    with open('/Users/hasan/Desktop/sdaia_bootcamp/secAI/utils/system_prompts.json', 'r') as f:
        SYSTEM_PROMPTS = json.load(f)
    try:
        from openai import OpenAI
        
        client = OpenAI(
            api_key=os.environ.get('OPENAI_API_KEY'),
        )
        
        system_prompt = SYSTEM_PROMPTS[task_type]
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        return f"Error occurred while getting LLM output: {str(e)}"

print(get_llm_output("Hello, how are you?", "task_classifier"))


==================================================
File: modules/input_handlers.py
==================================================

from modules.hand_gesture import process_gesture
from modules.llm_processor import get_llm_output
from modules.speech_to_text import speech_to_text

def handle_gesture(image, is_processing):
    if not is_processing or image is None:
        return -1
        
    task_num = process_gesture(image)
    print(f"Task number from gesture: {task_num}")
    return task_num if task_num in [1, 2, 3] else -1

def handle_audio(audio, is_processing):
    if not is_processing or audio is None:
        return -1, "No audio detected"
    
    try:
        transcribed_text = speech_to_text(audio)
        task_num = get_llm_output(transcribed_text, 'task_classifier')
        try:
            task_num = int(task_num)
            print(f"Task number from audio: {task_num}")
        except (ValueError, TypeError):
            task_num = -1
            
        return task_num, transcribed_text
                
    except Exception as e:
        error_msg = f"Error processing audio: {str(e)}"
        return -1, error_msg

